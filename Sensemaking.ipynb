{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###pdf parser\n",
    "###PyMuPDF: https://pymupdf.readthedocs.io/en/latest/\n",
    "import sys \n",
    "import fitz\n",
    "fname = \"xxx\"  # get document filename\n",
    "doc = fitz.open(fname)  # open document\n",
    "out = open(fname + \".txt\", \"wb\")  # open text output\n",
    "for page in doc:  # iterate the document pages\n",
    "    text = page.get_text().encode(\"utf8\")  # get plain text (is in UTF-8)\n",
    "    out.write(text)  # write text of page\n",
    "    out.write(bytes((12,)))  # write page delimiter (form feed 0x0C)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes local image set and returns absolute filepaths (.txt) for Handprint input + lookup table (.csv)\n",
    "##Matt Cook - 2021\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#inputs\n",
    "target =\"...\" #target image directory\n",
    "\n",
    "#outputs\n",
    "pathsOut = open(\"...txt\", \"w\") # for Handprint\n",
    "tableOut = \"...csv\"# transcription batch csv\n",
    "\n",
    "#dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#cross-check that DRS FILE-OSN values exist in target directory and add matches to pathsOut + dataframe\n",
    "for path in sorted(Path(target).rglob('*.jpg')):\n",
    "    absolute = (str(path.parent) + \"/\" + path.name) #absolute path for images\n",
    "    pathsOut.write(str(absolute)) #write paths to pathsOut\n",
    "    pathsOut.write(\"\\n\")\n",
    "    df = df.append({'FILENAME':path.stem,'IMG-PATH':absolute}, ignore_index=True) #append data frame  \n",
    "    print(\"image \" + path.stem + \" located \" + \"at \" + absolute) #console out\n",
    "\n",
    "#create new lookup table from dataframe\n",
    "with open(tableOut, mode = 'a') as f:\n",
    "    df.to_csv(f,index=False) #append tableOut with FILE-OSN and IMG-PATH values\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"lookup table created for collection\")\n",
    "pathsOut.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Handprint using paths from local file\n",
    "##Mike Hucka designed and implemented Handprint beginning in mid-2018.\n",
    "##installation instructions at https://github.com/caltechlibrary/handprint\n",
    "\n",
    "##generate Microsoft results\n",
    "!handprint --service microsoft -@\".../HTR_log.txt\" --from-file \"...txt\" --no-grid --extended --output \"...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "\n",
    "#input/output\n",
    "paths = '...txt'#plain text list of urls or filenames (pre-existing)\n",
    "target = '...MSFT' #HP outputs (pre-existing)\n",
    "textOut = '...txt'#Bag-of-Words output\n",
    "\n",
    "#declarations\n",
    "BoW = open(textOut, \"w\")\n",
    "#append bag-of-words with headers and transcriptions\n",
    "for path in sorted(Path(target).rglob('*.txt')):\n",
    "    absolute = (str(path.parent) + \"/\" + path.name)\n",
    "    contents = open(absolute, \"r\") \n",
    "    for line in contents.readlines():\n",
    "            print(\"\\n\")\n",
    "            b = TextBlob(line)\n",
    "            print(str(b.correct()))\n",
    "            BoW.write(str(b.correct()))\n",
    "    BoW.write(\"\\n\")\n",
    "BoW.close()\n",
    "print(\"\\n\")\n",
    "print(\"have a nice day\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Fuzzy Search Bag-of-Words\n",
    "##Matt Cook - 2021\n",
    "\n",
    "from fuzzy_search.fuzzy_phrase_searcher import FuzzyPhraseSearcher\n",
    "from fuzzy_search.fuzzy_phrase_model import PhraseModel\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "#declarations\n",
    "text = \".../Baptismal Records/Lag 1838-1869/Lag 1838-1869_BOW.txt\"\n",
    "target = '.../Baptismal Records/Lag 1838-1869/Lag 1838-1869_MSFT' #HP outputs (pre-existing)\n",
    "variants = []\n",
    "passages = open(\".../Baptismal Records/Lag 1838-1869/Lag 1838-1869_passages.txt\", \"w\") #output text\n",
    "\n",
    "#user input\n",
    "inputString = input(\"Fuzzy search document for keyword: \")\n",
    "passages.write(\"Fuzzy search document for keyword: \" + str(inputString))\n",
    "passages.write(\"\\n\")\n",
    "counter = 0\n",
    "\n",
    "#threshold configuration (Sarah to customize)\n",
    "config = {\n",
    "    #these thresholds work when there are few OCR errors\n",
    "    'char_match_threshold': 0.5,\n",
    "    'ngram_threshold': 0.5,\n",
    "    'skipgram_threshold': 0.3,\n",
    "    'levenshtein_threshold': 0.5,\n",
    "    'include_variants': False, # for phrases that have variant phrasings\n",
    "    'filter_distractors': False, # avoid matching with similar but different phrases\n",
    "    \"ignorecase\": False, # Is upper/lowercase a meaningful signal?\n",
    "    \"use_word_boundaries\": False,# should matches follow word boundaries?\n",
    "    \"max_length_variance\": 3, # matching string can be lower/shorter than prhase\n",
    "}\n",
    "\n",
    "# initialize a new searcher instance with the config\n",
    "fuzzy_searcher = FuzzyPhraseSearcher(config)\n",
    "phrase_model = PhraseModel(phrases=[inputString])\n",
    "fuzzy_searcher.index_phrase_model(phrase_model)\n",
    "\n",
    "#identify matches in the text using fuzzy search package\n",
    "BoW = open(text, \"r\")\n",
    "for match in fuzzy_searcher.find_matches(BoW.read()):\n",
    "    variant = match.json()\n",
    "    variant = variant['string']\n",
    "    variants.append(variant)\n",
    "print(\"Variants detected in bag-of-words include: \")\n",
    "passages.write(\"Variants detected in bag-of-words include: \")\n",
    "passages.write(\"\\n\\n\")\n",
    "print(\"\\n\")\n",
    "print(variants)\n",
    "passages.write(str(variants))\n",
    "passages.write(\"\\n\\n\")\n",
    "print(\"\\n\")\n",
    "BoW.close()\n",
    "\n",
    "print(\"variants appear on the following records:\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for path in sorted(Path(target).rglob('*.txt')):\n",
    "    header = path.stem.split('.')\n",
    "    absolute = (str(path.parent) + \"/\" + path.name)\n",
    "    contents = open(absolute, \"r\")\n",
    "    contents = contents.read()\n",
    "    for variant in variants:\n",
    "        if variant in contents:\n",
    "            passages.write(header[0] + \"\\n\" + contents)\n",
    "            passages.write(\"\\n\\n\")\n",
    "            print(\"Record: \" + header[0] + \"\\n\" + \"Transcription: \" + contents)\n",
    "            print(\"\\n\")\n",
    "passages.close()\n",
    "\n",
    "print(\"matched records saved to disk\")\n",
    "print(\"have a nice day\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Named entity recognition and frequency visualization of bag-of-transcriptions\n",
    "##Modified from \"named-entity-recognition\" repo by Mary Chester-Kadwell (https://github.com/mchesterkadwell/named-entity-recognition/blob/main/LICENSE)\n",
    "#Entity types: https://github.com/mchesterkadwell/named-entity-recognition/blob/main/2-named-entity-recognition-of-henslow-data.ipynb\n",
    "##Matt Cook - November 2021\n",
    "\n",
    "import spacy\n",
    "import it_core_news_sm\n",
    "from spacy import displacy\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "#declarations\n",
    "nlp = it_core_news_sm.load()\n",
    "text_file = Path('data', '...txt')\n",
    "entOut = open(\"...\", \"w\")\n",
    "    \n",
    "###named entity recognition\n",
    "with open(text_file, encoding=\"utf-8\") as file:\n",
    "    iliad = file.read()\n",
    "document = nlp(iliad)\n",
    "document.text\n",
    "entities = []\n",
    "for entity in document.ents:\n",
    "    if entity.label_ == \"GPE\" or \"NORP\": \n",
    "        entities.append(entity.text)\n",
    "        print(entity.text)\n",
    "entOut.write(str(entities))\n",
    "displacy.render(document, style=\"ent\")\n",
    "        \n",
    "#print high-frenquency entities\n",
    "word_freq = Counter(entity)\n",
    "common_words = word_freq.most_common(50)\n",
    "print(common_words)\n",
    "\n",
    "#Display the plot in the notebook with interactive controls and save plot to disk\n",
    "%matplotlib notebook\n",
    "words = [word for word,_ in common_words]\n",
    "freqs = [count for _,count in common_words]\n",
    "plt.title(\"Named Entities\")\n",
    "plt.xlabel(\"Entity type\")\n",
    "plt.ylabel(\"# of appearances\")\n",
    "plt.xticks(range(len(words)), [str(s) for s in words], rotation=90)\n",
    "plt.grid(b=True, which='major', color='#333333', linestyle='--', alpha=0.5)\n",
    "plt.gcf().subplots_adjust(bottom=0.35)\n",
    "plt.plot(freqs)\n",
    "plt.show()\n",
    "plt.savefig('....png', bbox_inches=\"tight\")\n",
    "\n",
    "#close files\n",
    "file.close()\n",
    "entOut.close()\n",
    "\n",
    "print(\"have a nice day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Topic Modeling (LDA) Longhand inputs\n",
    "###Cook 2021\n",
    "##portions of code from https://radimrehurek.com/gensim/auto_examples\n",
    "import os\n",
    "import sys\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "from gensim import models\n",
    "from gensim.models import LdaModel\n",
    "#import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#declarations\n",
    "doc = open(\"xxx\", \"r\")\n",
    "doc = doc.read()\n",
    "documents = [doc]\n",
    "\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [\n",
    "    [word for word in doc.lower().split() if word not in stoplist]\n",
    "    for doc in documents\n",
    "]\n",
    "#print(texts)\n",
    "\n",
    "#create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)\n",
    "\n",
    "#create corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(corpus)\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)\n",
    "\n",
    "doc.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
